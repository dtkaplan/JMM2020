[
["index.html", "Stats for Data Science Orientation Objective The new elements … Computing JMM 2020 Abstract Resources", " Stats for Data Science Daniel Kaplan 2020-01-03 Orientation These are notes for the MAA sponsored minicourse Stats for Data Science at the 2020 Joint Mathematics Meetings in Denver, Colorado. Part A, Thursday, 1:00 –3:00 pm Part B, Saturday, 1:00–3:00 pm. Objective I start with the presumption that you are interested in teaching data science and trying to sort out the possible relationship between introductory-level statistics and data science. My objective is to provide A complete outline of a plausible introductory course that genuinely engages data science while honestly covering core statistical topics in a statistical way. For practical reasons of time, I’ll emphasize the new elements and not topics that can be carried over from an existing course like sampling bias, random assignment, etc. Also, for reasons of time, I’ll focus only on the statistical concepts and methods and not on data-science topics such as wrangling, cleaning, etc. Put you in a position where you can teach such a course starting, let’s say, next fall. Not requiring you to master extensive new computational skills. Connected to many of the statistical topics you teach now … but considering which of those topics add genuine value and disregarding those that are merely included by tradition. The new elements … Streamline visualization. Adopt contemporary graphical modalities and by-pass the catalog of traditional formats. (15 minutes of this 4-hour mini-course) This frees up time in the your course, puts data at the center rather than theory, and avoids student confusion about why a particular style of graph is being used. Consistently use functions and effect sizes to describe relationships. (30 minutes of this 4-hour mini-course) The functions will have the form \\(y = f(x, z)\\) where \\(y\\) is the response variable \\(x\\) and \\(z\\) are explanatory variables. \\(z\\) plays the role of a covariate, that is, a variable not of direct interest that may color the observed relationship between \\(x\\) and \\(y\\). Effect sizes are slopes and differences. For this mathematically sophisticated audience, I’ll describe them as partial derivatives and partial differences: \\(\\partial y / \\partial x\\) for continuous \\(x\\) \\(\\Delta y\\) for categorical \\(x\\) I will treat model fitting as a calculator function: something that happens automatically once the inputs are provided. Generalize inference which allows the presentation to be streamlined and simplified and, at the same time, to be extended to settings not usually touched on in an introductory course. (30 minutes of this 4-hour mini-course) Ignore theoretical niceties that are of questionable utility or validity in real work, e.g. one-tailed tests unequal variance tests All inference will be about models and effect sizes. No methods, such as chi-squared, which produce only a p-value without an indication of the magnitude of the relationship. Pay heed to how much precision is actually needed in a result. For instance there’s no good reason to suggest that a p-value is meaningful past the second digit. So bypass calculations that worry about the third digit. Embrace causal reasoning from observational data. Teach students how to draw and recognize responsible conclusions about causation. (30 minutes) Integrate statistical findings with real-world decision making. reject the null? cost functions trade-offs using statistical results in the context of larger frameworks. Computing I want to put aside the question of how much computing should be taught in (or before) introductory statistics.1 Instead, I’ll focus on teaching a course using only: paper and pencil estimation and calculation no-coding web apps that make detailed graphics and do precise calculations Figure 0.1: Drawing graphics, model fitting, and statistical calculations can be carried out using a point-and-click web app. This is displayed in a regular browser window and can be plausibly used with a smart phone. We’ll use this app later in the mini-course. JMM 2020 Abstract As universities and colleges rush to offer courses and even degree programs in data science, it’s fair to wonder whether data science is genuinely new or is merely a rebranding of statistics. This mini-course will introduce participants to important and substantial ways that a statistics course that genuinely engages data science differs from traditional statistics.These include an emphasis on prediction, classification and causality rather than the traditional focus on estimation and significance. During the mini-course, we’ll work through both theoretical and computational exercises from a new book, Stats for Data Science (available at https://dtkaplan.github.io/SDS-book/preface.html). The workshop is appropriate for anyone from a newcomer to statistical computing to experts. Some small groups in the mini-course will choose to use mouse-driven “Little Apps” to display data-science oriented statistical concepts. Others will choose to work with interactive R tutorials based on modern modeling and graphics packages in R. Participants should bring a laptop or tablet. All work will be browser based; there’s no need to install new software. Resources Participant notes, comments, suggestions, questions, etc. Participant introductions Books Stats for Data Science textbook. The link is to the current draft of a textbook I am writing to explore how statistics can be taught in a way that genuinely embraces the typical goals of data-science practice. A Compact Guide to Classical Inference by Daniel Kaplan. This book deals with one small but important part making a mental and teaching transition from a conventional intro stat course into a course suitable for data science. In particular, the Compact Guide approaches statistical description using model functions and, with this basis, unifies and simplifies the inferential settings typically covered in inferential stats. All those traditional settings–difference of means and of proportions, simple regression, inference on contingency tables, one-way analysis of variance, two-way analysis of variance, multiple regression–are translated into a single test statistic, F, with a simple formula and simple interpretations. For instance, statistical “significance”2 is addressed by the simple question, is \\(F &gt; 4\\). Confidence intervals on differences and slopes are all shown to have the same form, proportional to \\(1 \\pm \\sqrt{4 / F}\\). Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim. For the introductory-course instructor who is not shy of using R with her class and who wants to touch on non-statistical aspects of data science such as data wrangling, this can be good choice for a textbook. The statistical topics are conventional, but the book wisely leaves the mean-median-mode stuff for an appendix. In the sense that the presentation of statistics is based on regression, I see this book as a kind updating for data science and recent developments in R of Statistical Modeling. (See next entry). Statistical Inference is not as radical as Stats for Data Science, but for many instructors that’s probably a good thing. There are exercises (“learning checks”) and solutions. Statistical Modeling: A Fresh Approach by Daniel Kaplan was my attempt, circa 2010, to re-imagine what can be done in an introductory statistics course to make the course more relevant to genuine practice, take confounding seriously, and provide room for student creativity in framing statistical questions. So, instead of “do I use t or chi-squared?” the question becomes “what covariates are relevant and what are the implications of including them in a statistical analysis?” Computer Age Statistical Inference by Bradley Efron and Trevor Hastie. This is a concise review of classical statistical inference that is much broader than the Compact Guide* and particularly oriented to deep theoretical limitations of classical inference and a couple of generations of work to overcome those limitations. The Book of Why by Judea Pearl and Dana Mackenzie. This is a fantastic introduction to causal inference which, yes, does go beyond the pat “correlation is not causation” or “no causation without experimentation.” The Theory that Would not Die by Sharon Bertsch McGrayne. The subtitle is “How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy” which aptly describe the book’s historical approach. This isn’t a textbook, but it is a good way to see why Bayes is important. Modern Data Science with R by Ben Baumer, Daniel Kaplan, and Nicholas Horton. This book covers a wide range of data science techniques, but wouldn’t be suitable for a statistics course. R for Data Science by Garrett Golemund and Hadley Wickham. Like Modern Data Science with R, it’s not a suitable book for a statistics course. But it’s an excellent (even canonical) choice to make sense of the recent generation of R data-science tools. Little Apps. These are web-based apps that provide statistical computing capabilities without coding. There are, of course, many other apps provided to the stat-ed community such as the Lock5 StatKey collections and Dan Adrian’s Happy Apps. Functions and F statistics. This is the Little App written specifically for the Compact Guide. It also happens to be the prototype for the next generation of Little Apps that are mobile-device ready. Regression models is a pre-cursor to the Functions and F statistics Little App. It explains the idea of model values, which are simply the values of a statistical model evaluated using the training data as input. Resampling and Bootstrapping demonstrates these ideas graphically. A few other Little Apps, developed as part of &lt;StatPREP.org&gt;, cover topics of the traditional intro course such as t-tests, the normal distribution and center and spread. Computing tutorials GET THE LIST FROM STATPREP.org. MAYBE WRITE ONE FOR THE COMPACT GUIDE? In my own courses I strongly emphasize technical computing and professional workflows.↩ Recently, Jeff Witmer at Oberlin College suggested replacing the misleading technical use of an everyday word with an utterly different meaning: significance. His suggestion is “discernible” and “discernibility”, as in “the difference is statistically discernible” or “one part of inference is statistical discernibility”.↩ "],
["stats-and-data-science.html", "Chapter 1 Stats and Data Science 1.1 The parable of the highway. 1.2 What’s different about data science?", " Chapter 1 Stats and Data Science 1.1 The parable of the highway. Figure 1.1: Urban driving in 1936. Statistics and automobiles have timelines that overlap nicely: Statistics Automobiles 1888 Francis Galton introduces the “co-relation” coefficient 1885 Karl Benz designs 4-stroke engine for use in his automobile 1908 William Gossett’s t statistic 1908 First Model T off Henry Ford’s production line 1925 ANOVA appears in Fisher’s Statistical Methods for Research Workers 1927 Ford Model A enters production There’s much more history to come, of course. Imagine you were asked to teach design a course about automobiles. You know a lot about automobiles and you’d have to make some choices about what to include. How much history? How much of the early technology? spark advance carburation tires with tubes steering tillers How much about contemporary technology fuel injection anti-lock breaks and stability control catalytic converters, oxygen sensors, … Automobile safety (starting about 1960) Road systems (e.g. Interstate highways, starting mid 1950s) Traffic jams and attempts to solve the problem Pollution and attempts to solve the problem Ride sharing Electric vehicles Self-driving vehicles If your goal is to orient students to the use and role of the automobile in the contemporary world, chances are you would skip carburation and tire-tube patching. Not much there to help contemporary automobile users. Probably you won’t be in the situation of designing a course about automobiles, even though you could teach one if need be. After all, you encounter automobiles every day. You probably drive them yourself. And you probably have been in the situation of choosing a car to buy, making the familiar trade-offs of performance, capacity, cost, fuel economy, etc. The situation you are actually in is that you have been asked to teach statistics. You might have taken a statistics course in college (but this is true for only a small minority of the math faculty teaching statistics). If you’re a mathematician, you probably have done little or no research applying statistics. Likely as well, you’ve never been in the position of having to choose statistical methods to use in studying a system of any complexity. And the statistics course that you do teach likely just barely reaches ANOVA as a topic. That is, your statistics course ends about 1925. Let’s give you 1926. That’s when Ronald Fisher introduced the 0.05 threshold, writing, “… it is convenient to draw the line at about the level at which we can say: ‘Either there is something in the treatment, or a coincidence has occurred such as does not occur more than once in twenty trials.’… If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the 2 per cent point), or one in a hundred (the 1 per cent point). Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fail to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance.” The world has changed in so many ways since 1926. Statistical methods have changed: model building is now central. Availability of data has changed: we rarely (except in textbooks) work with \\(n=5\\) measurement of a single variable. Graphical presentations have changed: we don’t need. Even the context has changed: the 0.05 p-value threshold, long scorned by professional statisticians, might have made sense in a world where a few professional scientists carried out bench-top experiments and routinely confirmed each other’s work. Today, of course, there are legions of scientists, huge numbers of publications, a professional penalty for “merely” repeating the work of others, and work involving tens or hundreds or even millions of variables. Statistics is no longer primarily a tool for the experimentalist, it is used to search for patterns in observational data and to predict outcomes from available inputs. And statistics is used to explore causality in observational systems, something that is discouraged strongly by textbooks, much like a driver in 1925 would be discouraged from driving across country in what is by any modern standard an outrageously unreliable and unsafe vehicle driven on roads that could hardly be called a road “system” with few supplies or comforts en route. So why are we teaching statistics courses about the Model T and Model A? It’s not because students need to calculate standard errors and quantiles of the t-distribution. It’s because we neither “drive” statistics or even see contemporary statistical vehicles or the road, safety, and traffic conditions for which they were developed. We need to discard the carburation of t distributions in favor of the fuel injection and stability control of modern computation and graphics. More fundamentally, we need to help move instructors from a system where they learn statistics from the textbooks they are teaching from, to a system where they are experienced with approaches to the kinds of problems faced by the data scientists and consumers of statistics today. Figure 1.2: A contemporary highway scene. The demands on cars and drivers are different today than in 1936. 1.2 What’s different about data science? Prediction Large data sets Multiple “tests” – e.g. batting averages Causality Decision making integrate the information from data into a broader framework Examples: Screening versus diagnostic tests Fuel economy. Not “Is fuel economy different at different speeds?” but “How different is it and what are the implications of this for my decision?” "],
["core-statistical-tools.html", "Chapter 2 Core statistical tools", " Chapter 2 Core statistical tools This will be paper-and-pencil introductions to the tools. "],
["graphics.html", "Chapter 3 Graphics 3.1 Graphical confusion: An introductory activity 3.2 A simplifying proposal 3.3 EXERCISES:", " Chapter 3 Graphics 3.1 Graphical confusion: An introductory activity Introductory statistics courses conventionally feature many types of graphics, e.g. histograms, stem-and-leaf plots, theoretical probability densities (often with tail probabilities annotated), bar charts, scatter plots, cross-tabulations, … The graphics in this table come from a nice, open-source textbook: OpenIntro Stats (4/e). This book is used successfully in many colleges, ranging from two-year colleges to elite, private, four-year schools. I paged through the book, capturing each new mode as I encountered it. As you look through the collection, note how often … it’s readily evident what a single “unit of observation” is. the axes correspond to a data variable one or another of the axes is not identified at all or the scale of the axis can be safely ignored. similar glyphs are used for completely different purposes Image modalities from OpenIntro Stats Not from Open Intro Not from Open Intro 3.2 A simplifying proposal Let’s reduce the number of graphical modes arranging things so … “Raw” data can be expected to be shown in a graphic. Each axis is always a variable, and the y axis is consistently used to display a response variable. The kind of glyph unequivocally identifies what is being displayed. 3.2.1 Data Layer The frame is something like this: gf_blank(Height ~ Age, data = NHANES) gf_blank(Poverty ~ HomeOwn, data = NHANES) gf_blank(Depressed ~ Work, data = NHANES) Data is always plotted with a ✺ Jittering and transparency are used to deal with overplotting. gf_point(Height ~ Age, data = NHANES, alpha = 0.3) gf_jitter(HomeOwn ~ Poverty, data = NHANES, alpha = 0.3, height = 0.2) gf_jitter(Depressed ~ Work, data = NHANES, alpha = 0.1, width = 0.2, height = 0.2) Adding in color and facetting, up to four variables can be shown, but relationships become progressively more difficult to read. 3.2.2 Model layer Glyph is a more-or-less horizontal line or curve showing the model output at each value of the explanatory variables. mod1 &lt;- lm(height ~ ns(mother,2) * sex, data = Galton) mod_shape &lt;- mod_eval(mod1, mother = seq(55,72,length = 100), interval = &quot;prediction&quot;) mod_shape2 &lt;- mod_eval(mod1, mother = seq(55,72,length = 100), interval = &quot;confidence&quot;) gf_point(height ~ mother | sex, data = Galton, alpha = 0.2) %&gt;% gf_line(model_output ~ mother | sex, data = mod_shape, size = 2) 3.2.3 Interval layer Glyph is either a ribbon or an I-bar. gf_point(height ~ mother | sex, data = Galton, alpha = 0.2) %&gt;% gf_ribbon(lower + upper ~ mother | sex, data = mod_shape, alpha = 0.2, inherit = FALSE) Note: In the context provided by the data, it’s always clear whether a band is a prediction interval or a confidence interval. gf_point(height ~ mother | sex, data = Galton, alpha = 0.1) %&gt;% gf_ribbon(lower + upper ~ mother | sex, data = mod_shape2, alpha = 0.4, inherit = FALSE) mod &lt;- lm(Height ~ Depressed, data = NHANES) mod_shape &lt;- mod_eval(mod, interval = &quot;prediction&quot;) Two distinct questions: Prediction: Does knowing the mother’s height meaningfully narrow a prediction of the adult child’s height? Description: Is the mother’s height connected with the adult child’s height? 3.2.4 Density layer The data points themselves indicate the joint probability density. Use another glyph to show conditional density: a violin: gf_jitter(Poverty ~ Depressed, data = NHANES, alpha = 0.03, width = 0.2) %&gt;% gf_violin(alpha = 0.1, fill = &quot;black&quot;) 3.3 EXERCISES: Is this a confidence interval or a prediction interval. For which group is low weight more common Weight ~ Smoker | Gender? Sketch out density violins. Which of these tells the story more clearly: Hourly number of rentals versus Time of Day and Client type and Weekday or Hourly number of rentals versus Weekday, Time of Day (color), and Client type. "],
["computation.html", "Chapter 4 Computation 4.1 No coding 4.2 Coding 4.3 Blank canvas", " Chapter 4 Computation 4.1 No coding Infrastructure: a browser. Can work on a smart phone. The Little Apps 4.2 Coding Infrastructure: a browser. Need a tablet+-sized screen and a keyboard. RStudio tutorials 4.3 Blank canvas Reproducible tools: Rmd "],
["models-and-effect-sizes.html", "Chapter 5 Models and effect sizes", " Chapter 5 Models and effect sizes Summarizing a relationship with a function We’ll start with models with a single degree of flexibility, that is \\(^\\circ{\\cal F} = 1\\). This includes all the settings covered in most introductory stats courses. Degrees of flexibility more generally. Explain why “degrees of freedom” is harder than degress of flexibility. You can count degrees of flexibility. "],
["generalizing-inference.html", "Chapter 6 Generalizing Inference 6.1 Variance: How much variation? 6.2 Model values: How much has been explained? 6.3 Basic discernibility 6.4 Confidence intervals (when \\(^\\circ\\!{\\cal F} = 1\\))", " Chapter 6 Generalizing Inference 6.1 Variance: How much variation? Average pairwise square differences between values. \\[\\frac{1}{n (n-1)}\\sum_{i \\neq j} |x_i - x_j|^2 = 2\\ \\mbox{Var}(x)\\] 6.2 Model values: How much has been explained? Pictures of model values and their variances compared to that of the response variable. Exercise: What is the variance of the response and of the model values? 6.3 Basic discernibility Note that I’m being much more mathy here than I would in teaching a typical class. The audience here is professional mathematicians, hence likely not too scared by algebraic notation. Is there any discernible relationship between the response and explanatory variables revealed by the model? Inputs from the model: \\(v_r\\), \\(v_m\\), \\(n\\), and degrees of flexibility \\(^\\circ{\\cal F}\\) Output: \\[\\mbox{F} = \\frac{n - (^\\circ{\\cal F} + 1)}{^\\circ{\\cal F}} \\frac{v_m}{v_r - v_m}\\] Interpretation: Is F \\(\\gtrapprox 4\\)?. Then a relationship is discernible. Given a base model and a proposed elaboration of that model, does the elaboration reveal new aspects of the relationship between the response and explanatory variables? Inputs from the model: \\(v_r\\) and \\(n\\) \\(v_m^{base}\\) and \\(v_m^{elab}\\), degrees of flexibility \\(^\\circ\\!{\\cal F}^{base}\\) and \\(^\\circ\\!{\\cal F}^{elab}\\) Output: \\[\\Delta \\mbox{F} = \\frac{n - (^\\circ\\!{\\cal F}^{elab} + 1)}{^\\circ\\!{\\cal F}^{elab} - ^\\circ\\!{\\cal F}^{base}} \\cdot \\frac{v_m^{elab} - v_m^{base}}{v_r - v_m^{elab}}\\] Interpretation: Is \\(\\Delta\\)F \\(\\gtrapprox 4\\)? Then a relationship is discernible.3 Notes: The special case of a model with \\(^\\circ{\\cal F} = 0\\) is called the Null Model and corresponds to the claim that there is no relationship between the explanatory variables and the response variable. In this special case, \\(\\mbox{F} = \\Delta \\mbox{F}\\). \\(\\Delta \\mbox{F} \\neq \\mbox{F}^{elab} - \\mbox{F}^{base}\\) 6.4 Confidence intervals (when \\(^\\circ\\!{\\cal F} = 1\\)) When \\(^\\circ\\!{\\cal F} = 1\\), there is only one explanatory variable and the modeling situation is one of these: difference between two groups slope of a regression line Either way, there is only one effect size: the difference or slope. Inputs: Effect size B F Output: Margin of error is \\(\\pm \\mbox{B} \\sqrt{4 / \\mbox{F}}\\) Interpretation: We wouldn’t be at all surprised if a much, much bigger study revealed an effect size within the confidence interval. - If we are comparing our study to another study, we’re only justified in claiming a contradiction when the two confidence intervals don’t overlap. Do we really need to refer to populations? Note that when \\(^\\circ\\!{\\cal F} \\geq 2\\), there is either more than one explanatory variable or more than one group in that explanatory variable or a non-straight-line regression (e.g. a polynomial). In none of these cases can the margin of error be deduced directly from F due to one or more of: effect size not constant multiple effect sizes collinearity among explanatory variables Instead of the simple formula based on F, confidence intervals can be based on a regression table or bootstrapping. Recall that I’m using discernible as a replacement for significant, as proposed by Jeff Witmer.↩ "],
["bayes-and-decision-making.html", "Chapter 7 Bayes (and decision making)", " Chapter 7 Bayes (and decision making) "],
["machine-learning.html", "Chapter 8 Machine learning", " Chapter 8 Machine learning Cross validation Bootstrapping "],
["causality-and-decision-making.html", "Chapter 9 Causality (and decision making)", " Chapter 9 Causality (and decision making) Recognize that we can do better than the abstinence-based curriculum. "],
["more-aspects-to-data-science.html", "Chapter 10 More aspects to data science", " Chapter 10 More aspects to data science Data science is not merely a rebranding of statistics. The scenario where statisticians would have come to lead the development of data science is plausible, but historically computer scientists and people from fields such as genetics, marketing, public health, medicine, remote sensing, etc. have played crucial roles. Whether or not data science ought to be considered part of the mathematical sciences, any genuine approach should be fundamentally based in realistic applications and the actual kinds of problems–especially decision making–that data science is used to address. For concise introductions to wrangling and visualization, see Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim or Data Computing by Daniel Kaplan and Matthew Beckman. I don’t know of a concise introduction to decision making from the statistics, mathematics, or computer science perspectives. (Please tell me if you do know of one!) But if you are willing to wade into the business literature, you would do well with How to Measure Anything by Douglas Hubbard. It even has a workbook. "]
]
